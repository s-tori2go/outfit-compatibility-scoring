{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108e8a78c8c98400",
   "metadata": {},
   "source": [
    "# imCLR-Style Contrastive Learning\n",
    "Setting up self-supervised visual representation learning on outfit images using a pre-trained ResNet50 encoder and SimCLR-style contrastive learning. This approach leverages both original and segmented images to create augmented pairs for contrastive training."
   ]
  },
  {
   "cell_type": "code",
   "id": "a77cf7a5f42c7d71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T07:11:34.905943Z",
     "start_time": "2025-07-09T07:11:25.831173Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from src import config"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "6875589c4be646fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T07:11:34.924560Z",
     "start_time": "2025-07-09T07:11:34.922596Z"
    }
   },
   "source": [
    "print(\"Using device:\", config.DEVICE)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "d80209da65d3bcd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T07:11:35.013647Z",
     "start_time": "2025-07-09T07:11:35.011873Z"
    }
   },
   "source": [
    "os.makedirs(config.CHECKPOINT_PATH, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "87a9a6fc6e621028",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T07:11:35.470013Z",
     "start_time": "2025-07-09T07:11:35.027285Z"
    }
   },
   "source": [
    "# Load pre-trained ResNet-50\n",
    "encoder = torchvision.models.resnet50(pretrained=True)\n",
    "for name, param in encoder.named_parameters():\n",
    "    if \"layer4\" in name or \"fc\" in name: # \"layer3\" in name or \n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "encoder.fc = torch.nn.Identity()\n",
    "encoder = encoder.to(config.DEVICE)\n",
    "# model_name = \"arize-ai/resnet-50-fashion-mnist-quality-drift\"\n",
    "# model = AutoModelForImageClassification.from_pretrained(model_name)\n",
    "# processor = AutoImageProcessor.from_pretrained(model_name)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tori/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/tori/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T07:11:35.498427Z",
     "start_time": "2025-07-09T07:11:35.495662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def contrastive_loss(z1, z2, temperature=0.5):\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "    representations = torch.cat([z1, z2], dim=0)\n",
    "    similarity_matrix = F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=2)\n",
    "    labels = torch.cat([torch.arange(z1.size(0)) for _ in range(2)], dim=0).to(config.DEVICE)\n",
    "    labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()\n",
    "    logits = similarity_matrix / temperature\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    return loss\n",
    "\n",
    "def nt_xent_loss(z1, z2, temperature=0.5):\n",
    "    batch_size = z1.size(0)\n",
    "    z = torch.cat([z1, z2], dim=0)\n",
    "    z = F.normalize(z, dim=1)\n",
    "    similarity_matrix = torch.matmul(z, z.T)\n",
    "    mask = torch.eye(2 * batch_size, dtype=torch.bool).to(z.device)\n",
    "    similarity_matrix = similarity_matrix.masked_fill(mask, -9e15)\n",
    "    positives = torch.cat([torch.arange(batch_size, 2*batch_size), torch.arange(0, batch_size)]).to(z.device)\n",
    "    logits = similarity_matrix / temperature\n",
    "    loss = F.cross_entropy(logits, positives)\n",
    "    return loss"
   ],
   "id": "34a7980f099a2b72",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "16853a1124f824c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T07:11:35.526967Z",
     "start_time": "2025-07-09T07:11:35.505375Z"
    }
   },
   "source": [
    "or_pos_dir = config.ORIGINAL_POS_OUTFITS_DIR\n",
    "or_neg_dir = config.ORIGINAL_NEG_OUTFITS_DIR\n",
    "seg_pos_dir = config.SEGMENTED_POS_OUTFITS_DIR\n",
    "seg_neg_dir = config.SEGMENTED_NEG_OUTFITS_DIR\n",
    "\n",
    "# Collect image paths and labels\n",
    "image_paths = []\n",
    "for class_idx, folder in enumerate([seg_neg_dir, seg_pos_dir]):\n",
    "    for img_name in os.listdir(folder):\n",
    "        if img_name.lower().endswith(config.IMAGE_FILE_EXTENSIONS):\n",
    "            img_path = os.path.join(folder, img_name)\n",
    "            image_paths.append(img_path)\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(image_paths)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T07:11:35.536852Z",
     "start_time": "2025-07-09T07:11:35.534474Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Images: {len(image_paths)}\")",
   "id": "f9d3c2a40836e97b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: 5347\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "8cd29cbecf9a0460",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T07:11:35.551104Z",
     "start_time": "2025-07-09T07:11:35.548856Z"
    }
   },
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        img1 = self.transform(image)\n",
    "        img2 = self.transform(image)\n",
    "        return img1, img2"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "715754ad71ddd0bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T07:11:35.563750Z",
     "start_time": "2025-07-09T07:11:35.561415Z"
    }
   },
   "source": [
    "# Image transformations\n",
    "contrastive_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.GaussianBlur(3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "447380b63be7da5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T07:11:35.572504Z",
     "start_time": "2025-07-09T07:11:35.570604Z"
    }
   },
   "source": [
    "# Load datasets\n",
    "train_dataset = CustomImageDataset(\n",
    "    image_paths=image_paths,\n",
    "    transform=contrastive_transform\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T07:11:35.580856Z",
     "start_time": "2025-07-09T07:11:35.578707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-4)\n",
    "start_epoch = 0\n",
    "train_losses = []    # Track training loss per epoch"
   ],
   "id": "be2c89ab4669cad3",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T07:11:35.793321Z",
     "start_time": "2025-07-09T07:11:35.587524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load checkpoint if resuming\n",
    "if config.RESUME_CHECKPOINT and os.path.exists(os.path.join(config.CHECKPOINT_PATH, f\"contrastive_encoder.pth\")):\n",
    "    checkpoint = torch.load(os.path.join(config.CHECKPOINT_PATH, f\"contrastive_encoder.pth\"))\n",
    "    encoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")"
   ],
   "id": "fbd7ad9a5bed2c8b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 100\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T04:07:40.316374Z",
     "start_time": "2025-07-08T16:35:30.564284Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 100, Loss: 6.8237\n",
      "⏱️ Took 173.23s\n",
      "Epoch 2 / 100, Loss: 6.3213\n",
      "⏱️ Took 342.28s\n",
      "Epoch 3 / 100, Loss: 6.1844\n",
      "⏱️ Took 519.77s\n",
      "Epoch 4 / 100, Loss: 6.1326\n",
      "⏱️ Took 700.48s\n",
      "Epoch 5 / 100, Loss: 6.1012\n",
      "⏱️ Took 1403.73s\n",
      "Epoch 6 / 100, Loss: 6.0745\n",
      "⏱️ Took 1622.85s\n",
      "Epoch 7 / 100, Loss: 6.0651\n",
      "⏱️ Took 1794.31s\n",
      "Epoch 8 / 100, Loss: 6.0307\n",
      "⏱️ Took 1974.78s\n",
      "Epoch 9 / 100, Loss: 6.0045\n",
      "⏱️ Took 2165.32s\n",
      "Epoch 10 / 100, Loss: 5.9931\n",
      "⏱️ Took 2640.63s\n",
      "Epoch 11 / 100, Loss: 5.9752\n",
      "⏱️ Took 2813.08s\n",
      "Epoch 12 / 100, Loss: 5.9519\n",
      "⏱️ Took 3025.52s\n",
      "Epoch 13 / 100, Loss: 5.9490\n",
      "⏱️ Took 3265.00s\n",
      "Epoch 14 / 100, Loss: 5.9434\n",
      "⏱️ Took 3488.93s\n",
      "Epoch 15 / 100, Loss: 5.9326\n",
      "⏱️ Took 3716.48s\n",
      "Epoch 16 / 100, Loss: 5.9353\n",
      "⏱️ Took 3923.03s\n",
      "Epoch 17 / 100, Loss: 5.9368\n",
      "⏱️ Took 4147.85s\n",
      "Epoch 18 / 100, Loss: 5.9210\n",
      "⏱️ Took 5060.39s\n",
      "Epoch 19 / 100, Loss: 5.9143\n",
      "⏱️ Took 5233.22s\n",
      "Epoch 20 / 100, Loss: 5.9066\n",
      "⏱️ Took 5426.39s\n",
      "Epoch 21 / 100, Loss: 5.8909\n",
      "⏱️ Took 5629.67s\n",
      "Epoch 22 / 100, Loss: 5.8985\n",
      "⏱️ Took 5862.12s\n",
      "Epoch 23 / 100, Loss: 5.8783\n",
      "⏱️ Took 6100.17s\n",
      "Epoch 24 / 100, Loss: 5.8957\n",
      "⏱️ Took 6346.82s\n",
      "Epoch 25 / 100, Loss: 5.8705\n",
      "⏱️ Took 6591.27s\n",
      "Epoch 26 / 100, Loss: 5.8692\n",
      "⏱️ Took 6820.38s\n",
      "Epoch 27 / 100, Loss: 5.8681\n",
      "⏱️ Took 7006.56s\n",
      "Epoch 28 / 100, Loss: 5.8566\n",
      "⏱️ Took 7194.95s\n",
      "Epoch 29 / 100, Loss: 5.8615\n",
      "⏱️ Took 7382.60s\n",
      "Epoch 30 / 100, Loss: 5.8675\n",
      "⏱️ Took 7570.26s\n",
      "Epoch 31 / 100, Loss: 5.8499\n",
      "⏱️ Took 7758.47s\n",
      "Epoch 32 / 100, Loss: 5.8427\n",
      "⏱️ Took 7947.61s\n",
      "Epoch 33 / 100, Loss: 5.8427\n",
      "⏱️ Took 8138.02s\n",
      "Epoch 34 / 100, Loss: 5.8542\n",
      "⏱️ Took 8327.53s\n",
      "Epoch 35 / 100, Loss: 5.8415\n",
      "⏱️ Took 8519.40s\n",
      "Epoch 36 / 100, Loss: 5.8245\n",
      "⏱️ Took 8710.87s\n",
      "Epoch 37 / 100, Loss: 5.8168\n",
      "⏱️ Took 8901.32s\n",
      "Epoch 38 / 100, Loss: 5.8340\n",
      "⏱️ Took 9101.36s\n",
      "Epoch 39 / 100, Loss: 5.8164\n",
      "⏱️ Took 9308.93s\n",
      "Epoch 40 / 100, Loss: 5.8169\n",
      "⏱️ Took 9503.46s\n",
      "Epoch 41 / 100, Loss: 5.8148\n",
      "⏱️ Took 9709.60s\n",
      "Epoch 42 / 100, Loss: 5.8021\n",
      "⏱️ Took 9942.85s\n",
      "Epoch 43 / 100, Loss: 5.8067\n",
      "⏱️ Took 10174.52s\n",
      "Epoch 44 / 100, Loss: 5.8173\n",
      "⏱️ Took 10419.79s\n",
      "Epoch 45 / 100, Loss: 5.8100\n",
      "⏱️ Took 10617.21s\n",
      "Epoch 46 / 100, Loss: 5.8050\n",
      "⏱️ Took 10812.64s\n",
      "Epoch 47 / 100, Loss: 5.7962\n",
      "⏱️ Took 11011.22s\n",
      "Epoch 48 / 100, Loss: 5.8039\n",
      "⏱️ Took 11225.77s\n",
      "Epoch 49 / 100, Loss: 5.8104\n",
      "⏱️ Took 11418.97s\n",
      "Epoch 50 / 100, Loss: 5.7869\n",
      "⏱️ Took 11613.03s\n",
      "Epoch 51 / 100, Loss: 5.7937\n",
      "⏱️ Took 11797.25s\n",
      "Epoch 52 / 100, Loss: 5.7773\n",
      "⏱️ Took 11993.13s\n",
      "Epoch 53 / 100, Loss: 5.7790\n",
      "⏱️ Took 12195.34s\n",
      "Epoch 54 / 100, Loss: 5.7838\n",
      "⏱️ Took 12388.43s\n",
      "Epoch 55 / 100, Loss: 5.7587\n",
      "⏱️ Took 12584.68s\n",
      "Epoch 56 / 100, Loss: 5.7717\n",
      "⏱️ Took 12771.84s\n",
      "Epoch 57 / 100, Loss: 5.7598\n",
      "⏱️ Took 12959.30s\n",
      "Epoch 58 / 100, Loss: 5.7709\n",
      "⏱️ Took 13791.21s\n",
      "Epoch 59 / 100, Loss: 5.7570\n",
      "⏱️ Took 16812.94s\n",
      "Epoch 60 / 100, Loss: 5.7804\n",
      "⏱️ Took 18614.62s\n",
      "Epoch 61 / 100, Loss: 5.7741\n",
      "⏱️ Took 19692.31s\n",
      "Epoch 62 / 100, Loss: 5.7735\n",
      "⏱️ Took 20099.01s\n",
      "Epoch 63 / 100, Loss: 5.7572\n",
      "⏱️ Took 20287.13s\n",
      "Epoch 64 / 100, Loss: 5.7690\n",
      "⏱️ Took 21457.96s\n",
      "Epoch 65 / 100, Loss: 5.7507\n",
      "⏱️ Took 25535.33s\n",
      "Epoch 66 / 100, Loss: 5.7434\n",
      "⏱️ Took 28400.08s\n",
      "Epoch 67 / 100, Loss: 5.7664\n",
      "⏱️ Took 28579.44s\n",
      "Epoch 68 / 100, Loss: 5.7388\n",
      "⏱️ Took 28773.74s\n",
      "Epoch 69 / 100, Loss: 5.7476\n",
      "⏱️ Took 28973.90s\n",
      "Epoch 70 / 100, Loss: 5.7578\n",
      "⏱️ Took 29179.08s\n",
      "Epoch 71 / 100, Loss: 5.7528\n",
      "⏱️ Took 29395.86s\n",
      "Epoch 72 / 100, Loss: 5.7453\n",
      "⏱️ Took 29618.75s\n",
      "Epoch 73 / 100, Loss: 5.7330\n",
      "⏱️ Took 29844.35s\n",
      "Epoch 74 / 100, Loss: 5.7411\n",
      "⏱️ Took 30073.00s\n",
      "Epoch 75 / 100, Loss: 5.7384\n",
      "⏱️ Took 30332.03s\n",
      "Epoch 76 / 100, Loss: 5.7345\n",
      "⏱️ Took 30590.97s\n",
      "Epoch 77 / 100, Loss: 5.7209\n",
      "⏱️ Took 30855.66s\n",
      "Epoch 78 / 100, Loss: 5.7367\n",
      "⏱️ Took 31122.26s\n",
      "Epoch 79 / 100, Loss: 5.7392\n",
      "⏱️ Took 31394.39s\n",
      "Epoch 80 / 100, Loss: 5.7221\n",
      "⏱️ Took 31680.34s\n",
      "Epoch 81 / 100, Loss: 5.7228\n",
      "⏱️ Took 31953.86s\n",
      "Epoch 82 / 100, Loss: 5.7207\n",
      "⏱️ Took 32929.48s\n",
      "Epoch 83 / 100, Loss: 5.7158\n",
      "⏱️ Took 34090.30s\n",
      "Epoch 84 / 100, Loss: 5.7106\n",
      "⏱️ Took 34341.32s\n",
      "Epoch 85 / 100, Loss: 5.7153\n",
      "⏱️ Took 34546.60s\n",
      "Epoch 86 / 100, Loss: 5.7073\n",
      "⏱️ Took 34772.98s\n",
      "Epoch 87 / 100, Loss: 5.7119\n",
      "⏱️ Took 35006.92s\n",
      "Epoch 88 / 100, Loss: 5.7145\n",
      "⏱️ Took 35253.11s\n",
      "Epoch 89 / 100, Loss: 5.7155\n",
      "⏱️ Took 35493.26s\n",
      "Epoch 90 / 100, Loss: 5.7074\n",
      "⏱️ Took 35735.80s\n",
      "Epoch 91 / 100, Loss: 5.6936\n",
      "⏱️ Took 35979.06s\n",
      "Epoch 92 / 100, Loss: 5.7042\n",
      "⏱️ Took 36227.62s\n",
      "Epoch 93 / 100, Loss: 5.6875\n",
      "⏱️ Took 36486.54s\n",
      "Epoch 94 / 100, Loss: 5.7006\n",
      "⏱️ Took 36741.20s\n",
      "Epoch 95 / 100, Loss: 5.6907\n",
      "⏱️ Took 37004.00s\n",
      "Epoch 96 / 100, Loss: 5.7108\n",
      "⏱️ Took 37260.64s\n",
      "Epoch 97 / 100, Loss: 5.6904\n",
      "⏱️ Took 37516.16s\n",
      "Epoch 98 / 100, Loss: 5.6930\n",
      "⏱️ Took 37771.42s\n",
      "Epoch 99 / 100, Loss: 5.6878\n",
      "⏱️ Took 39918.94s\n",
      "Epoch 100 / 100, Loss: 5.6809\n",
      "⏱️ Took 41529.73s\n"
     ]
    }
   ],
   "execution_count": 13,
   "source": [
    "start = time.time()\n",
    "\n",
    "for epoch in range(start_epoch, config.EPOCHS):\n",
    "    encoder.train()\n",
    "    total_loss = 0.0\n",
    "    for img1, img2 in train_loader:\n",
    "        img1, img2 = img1.to(config.DEVICE), img2.to(config.DEVICE)\n",
    "        \n",
    "        z1, z2 = encoder(img1), encoder(img2)\n",
    "        loss = contrastive_loss(z1, z2)\n",
    "\n",
    "        optimizer.zero_grad() # Reset gradients\n",
    "        loss.backward() # Compute gradients\n",
    "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1.0) # Clip gradients\n",
    "        optimizer.step() # Update parameters\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': encoder.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss.item()\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(config.CHECKPOINT_PATH, \"contrastive_encoder.pth\"))\n",
    "    print(f\"Epoch {epoch+1} / {config.EPOCHS}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    print(f\"⏱️ Took {time.time() - start:.2f}s\")\n"
   ],
   "id": "efa5e1a7924bcdb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6b9f696c050a55f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
