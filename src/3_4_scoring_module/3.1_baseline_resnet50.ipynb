{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108e8a78c8c98400",
   "metadata": {},
   "source": [
    "# Baseline ResNet-50\n",
    "Setting up a baseline ResNet-50 model for person-outfit binary classification (\"good\" vs. \"bad\")."
   ]
  },
  {
   "cell_type": "code",
   "id": "a77cf7a5f42c7d71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:02:51.444507Z",
     "start_time": "2025-07-05T09:02:51.432630Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import WeightedRandomSampler, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from src import config"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "6875589c4be646fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:02:51.464277Z",
     "start_time": "2025-07-05T09:02:51.459099Z"
    }
   },
   "source": [
    "print(\"Using device:\", config.DEVICE)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "d80209da65d3bcd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:03:05.424596Z",
     "start_time": "2025-07-05T09:03:05.421720Z"
    }
   },
   "source": [
    "os.makedirs(config.CHECKPOINT_PATH, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "87a9a6fc6e621028",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:03:05.779553Z",
     "start_time": "2025-07-05T09:03:05.432692Z"
    }
   },
   "source": [
    "# Load pre-trained ResNet-50\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "# model_name = \"arize-ai/resnet-50-fashion-mnist-quality-drift\"\n",
    "# model = AutoModelForImageClassification.from_pretrained(model_name)\n",
    "# processor = AutoImageProcessor.from_pretrained(model_name)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tori/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/tori/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "b28779797347bfda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:03:12.837601Z",
     "start_time": "2025-07-05T09:03:12.834982Z"
    }
   },
   "source": [
    "# Freeze all layers (base layers) except the last fully connected layer\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# Unfreeze last few layers\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layer4\" in name or \"fc\" in name: # \"layer3\" in name or \n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "b31da4f85b462498",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:03:21.342349Z",
     "start_time": "2025-07-05T09:03:21.337950Z"
    }
   },
   "source": [
    "# Modify for binary classification\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),  # Regularization\n",
    "    nn.Linear(256, 2)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "7e40e7d881f1a00b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:03:28.395268Z",
     "start_time": "2025-07-05T09:03:28.298365Z"
    }
   },
   "source": [
    "model = model.to(config.DEVICE)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "16853a1124f824c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:03:35.840203Z",
     "start_time": "2025-07-05T09:03:35.787265Z"
    }
   },
   "source": [
    "or_pos_dir = config.ORIGINAL_POS_OUTFITS_DIR\n",
    "or_neg_dir = config.ORIGINAL_NEG_OUTFITS_DIR\n",
    "seg_pos_dir = config.SEGMENTED_POS_OUTFITS_DIR\n",
    "seg_neg_dir = config.SEGMENTED_NEG_OUTFITS_DIR\n",
    "\n",
    "# Collect image paths and labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "for class_idx, folder in enumerate([seg_neg_dir, seg_pos_dir]):  # 0=negative, 1=positive\n",
    "    for img_name in os.listdir(folder):\n",
    "        if img_name.lower().endswith(config.IMAGE_FILE_EXTENSIONS):  # Filter images\n",
    "            img_path = os.path.join(folder, img_name)\n",
    "            image_paths.append(img_path)\n",
    "            labels.append(class_idx)\n",
    "\n",
    "for class_idx, folder in enumerate([or_neg_dir, or_pos_dir]):  # 0=negative, 1=positive\n",
    "    for img_name in os.listdir(folder):\n",
    "        if img_name.lower().endswith(config.IMAGE_FILE_EXTENSIONS):  # Filter images\n",
    "            img_path = os.path.join(folder, img_name)\n",
    "            image_paths.append(img_path)\n",
    "            labels.append(class_idx)\n",
    "\n",
    "import random\n",
    "\n",
    "# Combine image_paths and labels\n",
    "combined = list(zip(image_paths, labels))\n",
    "random.seed(42)\n",
    "random.shuffle(combined)\n",
    "\n",
    "# Unzip back to separate lists\n",
    "image_paths[:], labels[:] = zip(*combined)\n",
    "\n",
    "# Split with stratification (80% train, 20% validation)\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "    image_paths, \n",
    "    labels, \n",
    "    test_size=0.2, \n",
    "    stratify=labels,\n",
    "    random_state=42\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "43f76c85cd17ccf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:03:43.105344Z",
     "start_time": "2025-07-05T09:03:43.102929Z"
    }
   },
   "source": [
    "print(f\"Train: {sum(train_labels)} positives, {len(train_labels)-sum(train_labels)} negatives\")\n",
    "print(f\"Val: {sum(val_labels)} positives, {len(val_labels)-sum(val_labels)} negatives\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1306 positives, 7249 negatives\n",
      "Val: 326 positives, 1813 negatives\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "8cd29cbecf9a0460",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:03:57.269524Z",
     "start_time": "2025-07-05T09:03:57.266490Z"
    }
   },
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # Ensure RGB format\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "715754ad71ddd0bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:04:04.416472Z",
     "start_time": "2025-07-05T09:04:04.407199Z"
    }
   },
   "source": [
    "# Image transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "447380b63be7da5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:04:04.443695Z",
     "start_time": "2025-07-05T09:04:04.433105Z"
    }
   },
   "source": [
    "# Load datasets\n",
    "train_dataset = CustomImageDataset(\n",
    "    image_paths=train_paths,\n",
    "    labels=train_labels,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "val_dataset = CustomImageDataset(\n",
    "    image_paths=val_paths,\n",
    "    labels=val_labels,\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "class_sample_count = np.array([np.sum(labels == t) for t in np.unique(labels)])\n",
    "weights = 1. / class_sample_count\n",
    "samples_weight = np.array([weights[t] for t in labels])\n",
    "samples_weight = torch.from_numpy(samples_weight).double()\n",
    "\n",
    "sampler = WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:04:11.629139Z",
     "start_time": "2025-07-05T09:04:11.627555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_class_counts(dataset):\n",
    "    counts = [0, 0]  # [count_class0, count_class1]\n",
    "    for _, label in dataset:\n",
    "        # Handle both tensor and integer labels\n",
    "        if isinstance(label, torch.Tensor):\n",
    "            label = label.item()\n",
    "        counts[label] += 1\n",
    "    return counts"
   ],
   "id": "5482ef8878e3374b",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "b759880a51a6aa96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:04:48.147886Z",
     "start_time": "2025-07-05T09:04:18.709036Z"
    }
   },
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "class_counts = get_class_counts(train_dataset)\n",
    "weights = torch.tensor([\n",
    "    1.0 / max(class_counts[0], 1),  # Prevent division by zero\n",
    "    1.0 / max(class_counts[1], 1)\n",
    "], dtype=torch.float).to(config.DEVICE)\n",
    "\n",
    "# Loss function (binary classification)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Class 0: {class_counts[0]} samples\")\n",
    "print(f\"Class 1: {class_counts[1]} samples\")\n",
    "print(f\"Applied weights: {weights.cpu().numpy()}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 7249 samples\n",
      "Class 1: 1306 samples\n",
      "Applied weights: [0.00013795 0.0007657 ]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:04:55.312611Z",
     "start_time": "2025-07-05T09:04:55.307067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Optimizer with weight decay\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=6,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Early stopping setup\n",
    "best_val_loss = float('inf')\n",
    "patience, patience_counter = 10, 0\n",
    "\n",
    "# Checkpointing\n",
    "best_accuracy = 0.0\n",
    "best_f1 = 0\n",
    "start_epoch = 0"
   ],
   "id": "8dd82991400b3a4",
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "f59ac863a4b8501",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:05:02.743342Z",
     "start_time": "2025-07-05T09:05:02.321292Z"
    }
   },
   "source": [
    "# Load checkpoint if resuming\n",
    "if config.RESUME_CHECKPOINT and os.path.exists(os.path.join(config.CHECKPOINT_PATH, f\"best_model.pth\")):\n",
    "    checkpoint = torch.load(os.path.join(config.CHECKPOINT_PATH, f\"best_model.pth\"))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 48\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "efa5e1a7924bcdb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T12:20:36.048565Z",
     "start_time": "2025-07-05T09:12:50.636628Z"
    }
   },
   "source": [
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "previous_lr = optimizer.param_groups[0]['lr']\n",
    "train_losses = []    # Track training loss per epoch\n",
    "val_losses = []      # Track validation loss per epoch\n",
    "all_epoch_probs = [] # Store probabilities per epoch for ROC\n",
    "\n",
    "for epoch in range(start_epoch, config.EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(config.DEVICE), labels.to(config.DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels) \n",
    "        train_loss += loss.item() * inputs.size(0) # summing up all the elements\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Calculate average training loss\n",
    "    train_loss /= len(train_loader) # dividing by the total number of elements\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    total_samples = 0\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(config.DEVICE), labels.to(config.DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            total_samples += inputs.size(0)\n",
    "            \n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "            \n",
    "            # Store predictions/labels for metrics\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate average validation loss\n",
    "    val_loss /= total_samples\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (np.array(all_preds) == np.array(all_labels)).mean()\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    all_epoch_probs.append(all_probs)  # Store for later analysis\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Val Loss: {val_loss:.4f} | \"\n",
    "          f\"Accuracy: {accuracy:.2f} | \"\n",
    "          f\"Precision: {precision:.2f} | \"\n",
    "          f\"Recall: {recall:.2f} | \"\n",
    "          f\"F1: {f1:.2f} | \"\n",
    "          f\"AUC: {roc_auc:.2f}\")\n",
    "    \n",
    "    # LR scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    if current_lr < previous_lr:\n",
    "        print(f\"Epoch {epoch+1}: LR reduced to {current_lr}\")\n",
    "    previous_lr = current_lr\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss.item()\n",
    "    }\n",
    "\n",
    "    # Early stopping check\n",
    "    #if val_loss < best_val_loss:\n",
    "        # best_val_loss = val_loss\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        patience_counter = 0\n",
    "        torch.save(checkpoint, os.path.join(config.CHECKPOINT_PATH, f\"best_model_{epoch+1}.pth\"))\n",
    "        torch.save(checkpoint, os.path.join(config.CHECKPOINT_PATH, f\"best_model.pth\"))\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: Val Loss: 0.2883 | Accuracy: 0.93 | Precision: 0.71 | Recall: 0.88 | F1: 0.78 | AUC: 0.97\n",
      "Epoch 50: Val Loss: 0.3142 | Accuracy: 0.94 | Precision: 0.78 | Recall: 0.85 | F1: 0.81 | AUC: 0.97\n",
      "Epoch 51: Val Loss: 0.3098 | Accuracy: 0.94 | Precision: 0.77 | Recall: 0.87 | F1: 0.81 | AUC: 0.97\n",
      "Epoch 52: Val Loss: 0.3018 | Accuracy: 0.94 | Precision: 0.78 | Recall: 0.87 | F1: 0.82 | AUC: 0.97\n",
      "Epoch 53: Val Loss: 0.3012 | Accuracy: 0.94 | Precision: 0.77 | Recall: 0.87 | F1: 0.82 | AUC: 0.97\n",
      "Epoch 54: Val Loss: 0.3121 | Accuracy: 0.94 | Precision: 0.78 | Recall: 0.87 | F1: 0.82 | AUC: 0.97\n",
      "Epoch 55: Val Loss: 0.3195 | Accuracy: 0.94 | Precision: 0.76 | Recall: 0.88 | F1: 0.82 | AUC: 0.97\n",
      "Epoch 56: Val Loss: 0.3112 | Accuracy: 0.94 | Precision: 0.76 | Recall: 0.87 | F1: 0.81 | AUC: 0.97\n",
      "Epoch 56: LR reduced to 1.5625e-06\n",
      "Epoch 57: Val Loss: 0.3120 | Accuracy: 0.94 | Precision: 0.77 | Recall: 0.87 | F1: 0.82 | AUC: 0.97\n",
      "Epoch 58: Val Loss: 0.3221 | Accuracy: 0.94 | Precision: 0.77 | Recall: 0.88 | F1: 0.82 | AUC: 0.97\n",
      "Epoch 59: Val Loss: 0.3429 | Accuracy: 0.94 | Precision: 0.77 | Recall: 0.87 | F1: 0.82 | AUC: 0.97\n",
      "Epoch 60: Val Loss: 0.3313 | Accuracy: 0.94 | Precision: 0.77 | Recall: 0.86 | F1: 0.81 | AUC: 0.97\n",
      "Epoch 61: Val Loss: 0.3319 | Accuracy: 0.94 | Precision: 0.79 | Recall: 0.87 | F1: 0.83 | AUC: 0.97\n",
      "Epoch 62: Val Loss: 0.3368 | Accuracy: 0.95 | Precision: 0.80 | Recall: 0.87 | F1: 0.83 | AUC: 0.97\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 17\u001B[0m\n\u001B[1;32m     14\u001B[0m inputs, labels \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(config\u001B[38;5;241m.\u001B[39mDEVICE), labels\u001B[38;5;241m.\u001B[39mto(config\u001B[38;5;241m.\u001B[39mDEVICE)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(inputs)\n\u001B[1;32m     18\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels) \n\u001B[1;32m     19\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m*\u001B[39m inputs\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;66;03m# summing up all the elements\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torchvision/models/resnet.py:285\u001B[0m, in \u001B[0;36mResNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_impl(x)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torchvision/models/resnet.py:275\u001B[0m, in \u001B[0;36mResNet._forward_impl\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    273\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer1(x)\n\u001B[1;32m    274\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer2(x)\n\u001B[0;32m--> 275\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer3(x)\n\u001B[1;32m    276\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer4(x)\n\u001B[1;32m    278\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mavgpool(x)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:240\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    239\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 240\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m module(\u001B[38;5;28minput\u001B[39m)\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torchvision/models/resnet.py:155\u001B[0m, in \u001B[0;36mBottleneck.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    152\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(out)\n\u001B[1;32m    154\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv3(out)\n\u001B[0;32m--> 155\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn3(out)\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownsample \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    158\u001B[0m     identity \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownsample(x)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:193\u001B[0m, in \u001B[0;36m_BatchNorm.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    186\u001B[0m     bn_training \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_mean \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_var \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    188\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001B[39;00m\n\u001B[1;32m    190\u001B[0m \u001B[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001B[39;00m\n\u001B[1;32m    191\u001B[0m \u001B[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001B[39;00m\n\u001B[1;32m    192\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m--> 193\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mbatch_norm(\n\u001B[1;32m    194\u001B[0m     \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m    195\u001B[0m     \u001B[38;5;66;03m# If buffers are not to be tracked, ensure that they won't be updated\u001B[39;00m\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_mean\n\u001B[1;32m    197\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrack_running_stats\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    199\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_var \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrack_running_stats \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight,\n\u001B[1;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias,\n\u001B[1;32m    202\u001B[0m     bn_training,\n\u001B[1;32m    203\u001B[0m     exponential_average_factor,\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meps,\n\u001B[1;32m    205\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/functional.py:2822\u001B[0m, in \u001B[0;36mbatch_norm\u001B[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001B[0m\n\u001B[1;32m   2819\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m training:\n\u001B[1;32m   2820\u001B[0m     _verify_batch_size(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize())\n\u001B[0;32m-> 2822\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mbatch_norm(\n\u001B[1;32m   2823\u001B[0m     \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m   2824\u001B[0m     weight,\n\u001B[1;32m   2825\u001B[0m     bias,\n\u001B[1;32m   2826\u001B[0m     running_mean,\n\u001B[1;32m   2827\u001B[0m     running_var,\n\u001B[1;32m   2828\u001B[0m     training,\n\u001B[1;32m   2829\u001B[0m     momentum,\n\u001B[1;32m   2830\u001B[0m     eps,\n\u001B[1;32m   2831\u001B[0m     torch\u001B[38;5;241m.\u001B[39mbackends\u001B[38;5;241m.\u001B[39mcudnn\u001B[38;5;241m.\u001B[39menabled,\n\u001B[1;32m   2832\u001B[0m )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T12:20:46.059973Z",
     "start_time": "2025-07-05T12:20:44.968762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# 1. Plot loss curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training/Validation Loss')\n",
    "plt.savefig('training_validation_loss.png')\n",
    "plt.close()\n",
    "\n",
    "# 2. Precision-Recall Curve (from last epoch)\n",
    "plt.figure(figsize=(8, 6))\n",
    "precision, recall, thresholds = precision_recall_curve(all_labels, all_preds)\n",
    "pr_auc = auc(recall, precision)\n",
    "f1s = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "best_idx = np.argmax(f1s)\n",
    "best_threshold = thresholds[best_idx]\n",
    "print(f\"Best threshold: {best_threshold:.3f}, Best F1: {f1s[best_idx]:.3f}\")\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (area = {pr_auc:0.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.title('Precision-Recall Curve (last epoch)')\n",
    "plt.savefig('precision_recall_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# 3. Plot ROC Curve (from last epoch)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "         label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve (last epoch)')\n",
    "plt.legend()\n",
    "plt.savefig('roc_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# 3. Confusion Matrix (from last epoch)\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix (last epoch)')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, ['Negative', 'Positive'])\n",
    "plt.yticks(tick_marks, ['Negative', 'Positive'])\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# 4. Probability Distribution (MSE equivalent)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(np.array(all_probs)[np.array(all_labels) == 0], \n",
    "         bins=30, alpha=0.5, label='Negative Class')\n",
    "plt.hist(np.array(all_probs)[np.array(all_labels) == 1], \n",
    "         bins=30, alpha=0.5, label='Positive Class')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Predicted Probability Distribution')\n",
    "plt.legend()\n",
    "plt.savefig('probability_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# 5. Calibration\n",
    "prob_true, prob_pred = calibration_curve(all_labels, all_probs, n_bins=10)\n",
    "plt.plot(prob_pred, prob_true, marker='o')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('Mean predicted probability')\n",
    "plt.ylabel('Fraction of positives')\n",
    "plt.title('Calibration curve')\n",
    "plt.savefig('calibration_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# For actual calibration, fit a calibrator:\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "iso_reg = IsotonicRegression(out_of_bounds='clip')\n",
    "calibrated_probs = iso_reg.fit_transform(all_probs, all_labels)"
   ],
   "id": "e7722d9dc4cf1fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 1.000, Best F1: 0.829\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üìä **Visual Summary**\n",
    "| Metric      | Focus                  | Best Value | Worst Value | Ideal When...                |\n",
    "|-------------|------------------------|------------|-------------|------------------------------|\n",
    "| **Val Loss**| Prediction confidence  | 0.0        | ‚àû           | Monitoring training progress |\n",
    "| **Accuracy**| Overall correctness    | 1.0        | 0.0         | Balanced classes             |\n",
    "| **Precision**| False positives       | 1.0        | 0.0         | Costly false alarms          |\n",
    "| **Recall**  | False negatives        | 1.0        | 0.0         | Missing positives is bad     |\n",
    "| **F1**      | Precision-Recall tradeoff | 1.0     | 0.0         | Class imbalance exists       |\n",
    "\n",
    "### üîç **Interpretation**\n",
    "Validation shows:\n",
    "```\n",
    "Epoch 28: Val Loss: 0.3879 | Accuracy: 0.84 | Precision: 0.83 | Recall: 0.82 | F1: 0.83\n",
    "Epoch 5: Val Loss: 0.3822 | Accuracy: 0.84 | Precision: 0.87 | Recall: 0.79 | F1: 0.83\n",
    "Epoch 6: Val Loss: 0.3011 | Accuracy: 0.86 | Precision: 0.89 | Recall: 0.83 | F1: 0.86\n",
    "Epoch 38: Val Loss: 0.2807 | Accuracy: 0.91 | Precision: 0.92 | Recall: 0.90 | F1: 0.91\n",
    "```\n",
    "This means:\n",
    "1. Model is confident in correct predictions (low loss=39%)\n",
    "2. Shows strong overall performance (very good accuracy=84%)\n",
    "3. Is correct 83% of time, but has false positives (precision=83% means 17% of \"positive\" predictions are wrong)\n",
    "4. Catches most positives (high recall=82% means misses only 18% of true positives)\n",
    "5. F1=0.83 shows good balance\n",
    "\n",
    "**Actionable insight**: To improve precision, increase classification threshold.\n",
    "\n",
    "### üí° **When to Prioritize Which Metric**\n",
    "| Scenario                | Priority Metric  | Why                              |\n",
    "|-------------------------|------------------|----------------------------------|\n",
    "| Balanced classes        | Accuracy         | Simple overall measure           |\n",
    "| Costly false positives  | Precision        | Avoid false alarms               |\n",
    "| Costly false negatives  | Recall           | Don't miss critical cases        |\n",
    "| Class imbalance         | F1 Score         | Balances both error types        |\n",
    "| Training decisions      | Val Loss         | Most sensitive to model changes  |"
   ],
   "id": "d337fd5ecb29ccad"
  },
  {
   "cell_type": "code",
   "id": "dbbca8bbfe5a28a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T12:20:54.783063Z",
     "start_time": "2025-07-05T12:20:54.450353Z"
    }
   },
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(os.path.join(config.CHECKPOINT_PATH, f\"best_model.pth\"))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model = model.to(config.DEVICE)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    return val_transform(image).unsqueeze(0)  # Add batch dimension"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "8ff1c8710a49713e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T12:21:04.214177Z",
     "start_time": "2025-07-05T12:21:03.049994Z"
    }
   },
   "source": [
    "test_image_files = [\n",
    "    f for f in os.listdir(config.TEST_DIR)\n",
    "    if f.lower().endswith(config.IMAGE_FILE_EXTENSIONS)\n",
    "]\n",
    "\n",
    "test_image_files.sort()\n",
    "\n",
    "correct_count = 0\n",
    "faulty_count = 0\n",
    "\n",
    "for i, image_file in enumerate(test_image_files):\n",
    "    image_path = os.path.join(config.TEST_DIR, image_file)\n",
    "    image_tensor = preprocess_image(image_path).to(config.DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities, 1).item()\n",
    "\n",
    "    class_names = ['bad', 'good']  # 0=negative, 1=positive\n",
    "    print(f\"Image {i+1}: {image_file}\")\n",
    "    print(f\"Predicted: {class_names[predicted_class]}\")\n",
    "    print(f\"Confidence: {probabilities[0][predicted_class].item():.2%}\")\n",
    "    \n",
    "    # Condition checks\n",
    "    if (class_names[predicted_class] == 'good' and 'good' in image_file) or \\\n",
    "       (class_names[predicted_class] == 'bad' and 'bad' in image_file):\n",
    "        print(\"classification correct\\n\")\n",
    "        correct_count += 1\n",
    "    else:\n",
    "        print(\"classification faulty\\n\")\n",
    "        faulty_count += 1\n",
    "\n",
    "not_rated = 0\n",
    "\n",
    "# Final summary\n",
    "print(f\"Total correct classifications: {correct_count} out of {correct_count + faulty_count - not_rated} ({correct_count / (correct_count + faulty_count - not_rated) * 100:.2f}%)\")\n",
    "print(f\"Total faulty classifications: {faulty_count - not_rated} out of {correct_count + faulty_count - not_rated} ({(faulty_count - not_rated) / (correct_count + faulty_count - not_rated) * 100:.2f}%)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1: 10_good_o.jpg\n",
      "Predicted: good\n",
      "Confidence: 99.62%\n",
      "classification correct\n",
      "\n",
      "Image 2: 11_good_o.jpg\n",
      "Predicted: good\n",
      "Confidence: 99.12%\n",
      "classification correct\n",
      "\n",
      "Image 3: 12_good_o.jpg\n",
      "Predicted: good\n",
      "Confidence: 80.69%\n",
      "classification correct\n",
      "\n",
      "Image 4: 13_bad_o.jpg\n",
      "Predicted: bad\n",
      "Confidence: 100.00%\n",
      "classification correct\n",
      "\n",
      "Image 5: 14_bad_o.jpg\n",
      "Predicted: bad\n",
      "Confidence: 99.98%\n",
      "classification correct\n",
      "\n",
      "Image 6: 15_good_o.jpg\n",
      "Predicted: bad\n",
      "Confidence: 99.80%\n",
      "classification faulty\n",
      "\n",
      "Image 7: 16_good_o.jpg\n",
      "Predicted: bad\n",
      "Confidence: 99.51%\n",
      "classification faulty\n",
      "\n",
      "Image 8: 17_bad_o.jpg\n",
      "Predicted: good\n",
      "Confidence: 94.74%\n",
      "classification faulty\n",
      "\n",
      "Image 9: 18_bad_o.jpg\n",
      "Predicted: bad\n",
      "Confidence: 99.92%\n",
      "classification correct\n",
      "\n",
      "Image 10: 19_good_o.jpg\n",
      "Predicted: good\n",
      "Confidence: 99.88%\n",
      "classification correct\n",
      "\n",
      "Image 11: 1_good_o.jpg\n",
      "Predicted: good\n",
      "Confidence: 67.17%\n",
      "classification correct\n",
      "\n",
      "Image 12: 1_good_s.jpg\n",
      "Predicted: good\n",
      "Confidence: 53.38%\n",
      "classification correct\n",
      "\n",
      "Image 13: 20_good_o.jpg\n",
      "Predicted: good\n",
      "Confidence: 88.87%\n",
      "classification correct\n",
      "\n",
      "Image 14: 21_bad_o.jpg\n",
      "Predicted: bad\n",
      "Confidence: 99.99%\n",
      "classification correct\n",
      "\n",
      "Image 15: 22_good_o.jpg\n",
      "Predicted: good\n",
      "Confidence: 99.05%\n",
      "classification correct\n",
      "\n",
      "Image 16: 23_bad_o.jpg\n",
      "Predicted: bad\n",
      "Confidence: 98.40%\n",
      "classification correct\n",
      "\n",
      "Image 17: 24_bad_o.jpg\n",
      "Predicted: good\n",
      "Confidence: 99.94%\n",
      "classification faulty\n",
      "\n",
      "Image 18: 25_good_o.jpg\n",
      "Predicted: bad\n",
      "Confidence: 100.00%\n",
      "classification faulty\n",
      "\n",
      "Image 19: 26_bad_o.jpg\n",
      "Predicted: good\n",
      "Confidence: 95.61%\n",
      "classification faulty\n",
      "\n",
      "Image 20: 27_good_o.jpg\n",
      "Predicted: bad\n",
      "Confidence: 100.00%\n",
      "classification faulty\n",
      "\n",
      "Image 21: 28_bad_o.jpg\n",
      "Predicted: good\n",
      "Confidence: 74.14%\n",
      "classification faulty\n",
      "\n",
      "Image 22: 29_bad_o.jpg\n",
      "Predicted: bad\n",
      "Confidence: 99.98%\n",
      "classification correct\n",
      "\n",
      "Image 23: 2_bad_o.jpg\n",
      "Predicted: bad\n",
      "Confidence: 100.00%\n",
      "classification correct\n",
      "\n",
      "Image 24: 2_bad_s.jpg\n",
      "Predicted: bad\n",
      "Confidence: 98.46%\n",
      "classification correct\n",
      "\n",
      "Image 25: 30_bad_o.jpg\n",
      "Predicted: good\n",
      "Confidence: 83.36%\n",
      "classification faulty\n",
      "\n",
      "Image 26: 31_good_o.jpg\n",
      "Predicted: bad\n",
      "Confidence: 65.89%\n",
      "classification faulty\n",
      "\n",
      "Image 27: 32_good_o.jpg\n",
      "Predicted: bad\n",
      "Confidence: 99.98%\n",
      "classification faulty\n",
      "\n",
      "Image 28: 33_bad_o.jpg\n",
      "Predicted: bad\n",
      "Confidence: 98.60%\n",
      "classification correct\n",
      "\n",
      "Image 29: 34_good_o.jpg\n",
      "Predicted: bad\n",
      "Confidence: 99.68%\n",
      "classification faulty\n",
      "\n",
      "Image 30: 35_good_o.jpg\n",
      "Predicted: good\n",
      "Confidence: 92.90%\n",
      "classification correct\n",
      "\n",
      "Image 31: 3_bad_o.jpg\n",
      "Predicted: bad\n",
      "Confidence: 54.25%\n",
      "classification correct\n",
      "\n",
      "Image 32: 3_bad_s.jpg\n",
      "Predicted: bad\n",
      "Confidence: 98.38%\n",
      "classification correct\n",
      "\n",
      "Image 33: 5_bad_o.jpg\n",
      "Predicted: bad\n",
      "Confidence: 99.99%\n",
      "classification correct\n",
      "\n",
      "Image 34: 5_bad_s.jpg\n",
      "Predicted: bad\n",
      "Confidence: 100.00%\n",
      "classification correct\n",
      "\n",
      "Image 35: 6_good_o.jpg\n",
      "Predicted: good\n",
      "Confidence: 99.66%\n",
      "classification correct\n",
      "\n",
      "Image 36: 6_good_s.jpg\n",
      "Predicted: good\n",
      "Confidence: 98.08%\n",
      "classification correct\n",
      "\n",
      "Image 37: 7_bad_o.jpg\n",
      "Predicted: bad\n",
      "Confidence: 99.97%\n",
      "classification correct\n",
      "\n",
      "Image 38: 7_bad_s.jpg\n",
      "Predicted: bad\n",
      "Confidence: 99.98%\n",
      "classification correct\n",
      "\n",
      "Image 39: 8_bad_o.jpg\n",
      "Predicted: bad\n",
      "Confidence: 100.00%\n",
      "classification correct\n",
      "\n",
      "Image 40: 9_good_o.jpg\n",
      "Predicted: good\n",
      "Confidence: 83.08%\n",
      "classification correct\n",
      "\n",
      "Total correct classifications: 28 out of 40 (70.00%)\n",
      "Total faulty classifications: 12 out of 40 (30.00%)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "efe554bfd6894d5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T07:08:12.498967Z",
     "start_time": "2025-06-29T20:59:15.479651Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
