{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "108e8a78c8c98400",
   "metadata": {},
   "source": [
    "# imCLR-Style Contrastive Learning\n",
    "Setting up self-supervised visual representation learning on outfit images using a pre-trained ResNet50 encoder and SimCLR-style contrastive learning. This approach leverages both original and segmented images to create augmented pairs for contrastive training."
   ]
  },
  {
   "cell_type": "code",
   "id": "a77cf7a5f42c7d71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T11:39:40.836984Z",
     "start_time": "2025-07-09T11:39:35.315973Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from src import config"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "6875589c4be646fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T11:39:40.855435Z",
     "start_time": "2025-07-09T11:39:40.853424Z"
    }
   },
   "source": [
    "print(\"Using device:\", config.DEVICE)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T11:39:41.392753Z",
     "start_time": "2025-07-09T11:39:40.905473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load pre-trained ResNet-50\n",
    "encoder = torchvision.models.resnet50(pretrained=True)\n",
    "for name, param in encoder.named_parameters():\n",
    "    if \"layer4\" in name or \"fc\" in name: # \"layer3\" in name or \n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "encoder.fc = torch.nn.Identity()\n",
    "encoder = encoder.to(config.DEVICE)"
   ],
   "id": "87a9a6fc6e621028",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tori/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/tori/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "715754ad71ddd0bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T11:39:41.419722Z",
     "start_time": "2025-07-09T11:39:41.417167Z"
    }
   },
   "source": [
    "# Image transformations\n",
    "contrastive_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.GaussianBlur(3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T11:39:41.431259Z",
     "start_time": "2025-07-09T11:39:41.428194Z"
    }
   },
   "cell_type": "code",
   "source": "optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-4)",
   "id": "be2c89ab4669cad3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T11:39:41.626733Z",
     "start_time": "2025-07-09T11:39:41.440446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load checkpoint if resuming\n",
    "if config.RESUME_CHECKPOINT and os.path.exists(os.path.join(config.CHECKPOINT_PATH, f\"contrastive_encoder.pth\")):\n",
    "    checkpoint = torch.load(os.path.join(config.CHECKPOINT_PATH, f\"contrastive_encoder.pth\"))\n",
    "    encoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")"
   ],
   "id": "fbd7ad9a5bed2c8b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 100\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T11:39:51.279246Z",
     "start_time": "2025-07-09T11:39:51.274871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomVisualizationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ],
   "id": "98e92d9b1c134b7a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T11:39:58.389643Z",
     "start_time": "2025-07-09T11:39:58.381999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "or_pos_dir = config.ORIGINAL_POS_OUTFITS_DIR\n",
    "or_neg_dir = config.ORIGINAL_NEG_OUTFITS_DIR\n",
    "seg_pos_dir = config.SEGMENTED_POS_OUTFITS_DIR\n",
    "seg_neg_dir = config.SEGMENTED_NEG_OUTFITS_DIR\n",
    "\n",
    "image_paths = []\n",
    "labels = []\n",
    "for class_idx, folder in enumerate([seg_neg_dir, seg_pos_dir]):  # 0=negative, 1=positive\n",
    "    for img_name in os.listdir(folder):\n",
    "        if img_name.lower().endswith(config.IMAGE_FILE_EXTENSIONS):  # Filter images\n",
    "            img_path = os.path.join(folder, img_name)\n",
    "            image_paths.append(img_path)\n",
    "            labels.append(class_idx)\n",
    "\n",
    "dataset = CustomVisualizationDataset(image_paths, labels, contrastive_transform)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False)"
   ],
   "id": "4cb73b58488b57ff",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T11:41:14.774888Z",
     "start_time": "2025-07-09T11:40:05.401178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoder.eval()\n",
    "embeddings = []\n",
    "labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, batch_labels in loader:\n",
    "        images = images.to(config.DEVICE)\n",
    "        z = encoder(images)\n",
    "        embeddings.append(z.cpu().numpy())\n",
    "        labels_list.extend(batch_labels.numpy())\n",
    "\n",
    "embeddings = np.concatenate(embeddings, axis=0)\n",
    "labels_array = np.array(labels_list)"
   ],
   "id": "2334931984607a47",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T12:44:07.753273Z",
     "start_time": "2025-07-09T12:44:01.894118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import umap\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "umap_result = umap.UMAP(n_components=2, random_state=42).fit_transform(embeddings)\n",
    "\n",
    "color_map = np.array(['blue', 'orange']) \n",
    "plt.figure(figsize=(8, 8))\n",
    "scatter = plt.scatter(umap_result[:, 0], umap_result[:, 1], c=color_map[labels], cmap='coolwarm', alpha=0.7)\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', label='Bad Outfit', markerfacecolor='blue', markersize=10),\n",
    "    Line2D([0], [0], marker='o', color='w', label='Good Outfit', markerfacecolor='orange', markersize=10)\n",
    "]\n",
    "plt.legend(handles=legend_elements, title='Outfit Quality')\n",
    "\n",
    "plt.title('2D UMAP Visualization of Image Embeddings')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.savefig('embeddings-2d-visualization.png')\n",
    "plt.close()"
   ],
   "id": "a9aafc16f460cd19",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tori/miniconda3/lib/python3.12/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "/var/folders/6l/t3g7w2rj1pz06k2_mpf_fwsh0000gn/T/ipykernel_96270/1424776542.py:8: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n",
      "  scatter = plt.scatter(umap_result[:, 0], umap_result[:, 1], c=color_map[labels], cmap='coolwarm', alpha=0.7)\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-09T12:44:23.100526Z",
     "start_time": "2025-07-09T12:44:17.038258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "umap_result = umap.UMAP(n_components=3, random_state=42).fit_transform(embeddings)\n",
    "\n",
    "color_map = np.array(['blue', 'orange']) \n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Create scatter plot\n",
    "scatter = ax.scatter(\n",
    "    umap_result[:, 0], umap_result[:, 1], umap_result[:, 2],\n",
    "    c=color_map[labels], alpha=0.7\n",
    ")\n",
    "\n",
    "# Add axis labels\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.set_zlabel('Dimension 3')\n",
    "plt.title('3D UMAP Visualization of Embeddings')\n",
    "\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', label='Bad Outfit', markerfacecolor='blue', markersize=10),\n",
    "    Line2D([0], [0], marker='o', color='w', label='Good Outfit', markerfacecolor='orange', markersize=10)\n",
    "]\n",
    "ax.legend(handles=legend_elements, title='Outfit Quality')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('embeddings-3d-visualization.png')\n",
    "plt.close()"
   ],
   "id": "9b8b90bd5b7ba2e3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tori/miniconda3/lib/python3.12/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "encoder.eval()\n",
    "embeddings = []\n",
    "labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, batch_labels in loader:\n",
    "        images = images.to(config.DEVICE)\n",
    "        z = encoder(images)\n",
    "        embeddings.append(z.cpu().numpy())\n",
    "        labels_list.extend(batch_labels.numpy())\n",
    "\n",
    "embeddings = np.concatenate(embeddings, axis=0)\n",
    "labels_array = np.array(labels_list)"
   ],
   "id": "b0d0fabbd2c82af6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_image_files = [\n",
    "    f for f in os.listdir(config.TEST_DIR)\n",
    "    if f.lower().endswith(config.IMAGE_FILE_EXTENSIONS)\n",
    "]\n",
    "\n",
    "test_image_files.sort()\n",
    "\n",
    "correct_count = 0\n",
    "faulty_count = 0\n",
    "\n",
    "for i, image_file in enumerate(test_image_files):\n",
    "    image_path = os.path.join(config.TEST_DIR, image_file)\n",
    "    image_tensor = preprocess_image(image_path).to(config.DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities, 1).item()\n",
    "\n",
    "    class_names = ['bad', 'good']  # 0=negative, 1=positive\n",
    "    print(f\"Image {i+1}: {image_file}\")\n",
    "    print(f\"Predicted: {class_names[predicted_class]}\")\n",
    "    print(f\"Confidence: {probabilities[0][predicted_class].item():.2%}\")\n",
    "    \n",
    "    # Condition checks\n",
    "    if (class_names[predicted_class] == 'good' and 'good' in image_file) or \\\n",
    "       (class_names[predicted_class] == 'bad' and 'bad' in image_file):\n",
    "        print(\"classification correct\\n\")\n",
    "        correct_count += 1\n",
    "    else:\n",
    "        print(\"classification faulty\\n\")\n",
    "        faulty_count += 1\n",
    "\n",
    "not_rated = 0\n",
    "\n",
    "# Final summary\n",
    "print(f\"Total correct classifications: {correct_count} out of {correct_count + faulty_count - not_rated} ({correct_count / (correct_count + faulty_count - not_rated) * 100:.2f}%)\")\n",
    "print(f\"Total faulty classifications: {faulty_count - not_rated} out of {correct_count + faulty_count - not_rated} ({(faulty_count - not_rated) / (correct_count + faulty_count - not_rated) * 100:.2f}%)\")"
   ],
   "id": "64109a5c37de99b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ecc741f07a1d4e84"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
